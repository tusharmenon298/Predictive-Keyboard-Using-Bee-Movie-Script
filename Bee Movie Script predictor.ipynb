{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bee Movie Script Predictor\n",
    "In this project, we are gonna use the entire Bee Movie script to create a predictive keyboard\n",
    "We use the Recurrent Neural Network for this purpose. This model was chosen because it provides a way to examine the previous input. LSTM, a special kind of RNN is also used for this purpose. The LSTM provides the mechanism to preserve the errors that can be backpropagated through time and layers which helps to reduce vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense,Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the dataset\n",
    "First we are going to read the .txt file with the entire script then split the data into a list without any special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script Length 55315\n"
     ]
    }
   ],
   "source": [
    "text = open(\"beeMovie.txt\").read().lower()\n",
    "print('Script Length', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['according',\n",
       " 'to',\n",
       " 'all',\n",
       " 'known',\n",
       " 'laws',\n",
       " 'of',\n",
       " 'aviation',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(text)\n",
    "words = [item for item in words if item.isalpha()] #Any item in the list that contains a numbers\n",
    "words[0:10] #To see the first 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna make a unique sorted word list\n",
    "Then we are going to make a dictionary. The key is going to be the words and the corresponding position is going to be the value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'able': 1,\n",
       " 'abort': 2,\n",
       " 'aborting': 3,\n",
       " 'about': 4,\n",
       " 'absolutely': 5,\n",
       " 'absurd': 6,\n",
       " 'according': 7,\n",
       " 'account': 8,\n",
       " 'across': 9,\n",
       " 'action': 10,\n",
       " 'actor': 11,\n",
       " 'actual': 12,\n",
       " 'actually': 13,\n",
       " 'adam': 14,\n",
       " 'addicted': 15,\n",
       " 'adjusted': 16,\n",
       " 'adrenaline': 17,\n",
       " 'ads': 18,\n",
       " 'advancement': 19,\n",
       " 'advantage': 20,\n",
       " 'advisory': 21,\n",
       " 'affect': 22,\n",
       " 'affects': 23,\n",
       " 'affirmative': 24,\n",
       " 'afraid': 25,\n",
       " 'after': 26,\n",
       " 'afternoon': 27,\n",
       " 'aftertaste': 28,\n",
       " 'again': 29,\n",
       " 'against': 30,\n",
       " 'agreed': 31,\n",
       " 'ahead': 32,\n",
       " 'aim': 33,\n",
       " 'aiming': 34,\n",
       " 'air': 35,\n",
       " 'airport': 36,\n",
       " 'alaska': 37,\n",
       " 'alert': 38,\n",
       " 'alive': 39,\n",
       " 'all': 40,\n",
       " 'allergic': 41,\n",
       " 'allow': 42,\n",
       " 'almost': 43,\n",
       " 'alone': 44,\n",
       " 'already': 45,\n",
       " 'also': 46,\n",
       " 'always': 47,\n",
       " 'am': 48,\n",
       " 'amazing': 49,\n",
       " 'amen': 50,\n",
       " 'amusement': 51,\n",
       " 'an': 52,\n",
       " 'anchor': 53,\n",
       " 'and': 54,\n",
       " 'angel': 55,\n",
       " 'anger': 56,\n",
       " 'angry': 57,\n",
       " 'animal': 58,\n",
       " 'animals': 59,\n",
       " 'another': 60,\n",
       " 'ant': 61,\n",
       " 'antennae': 62,\n",
       " 'antennas': 63,\n",
       " 'antonio': 64,\n",
       " 'anxiously': 65,\n",
       " 'any': 66,\n",
       " 'anybody': 67,\n",
       " 'anyone': 68,\n",
       " 'anything': 69,\n",
       " 'anyway': 70,\n",
       " 'anywhere': 71,\n",
       " 'approach': 72,\n",
       " 'appropriate': 73,\n",
       " 'approved': 74,\n",
       " 'are': 75,\n",
       " 'area': 76,\n",
       " 'aren': 77,\n",
       " 'around': 78,\n",
       " 'arousing': 79,\n",
       " 'artie': 80,\n",
       " 'artificial': 81,\n",
       " 'as': 82,\n",
       " 'ask': 83,\n",
       " 'asked': 84,\n",
       " 'aspect': 85,\n",
       " 'associate': 86,\n",
       " 'assume': 87,\n",
       " 'assuming': 88,\n",
       " 'at': 89,\n",
       " 'athletic': 90,\n",
       " 'attack': 91,\n",
       " 'attempting': 92,\n",
       " 'attendant': 93,\n",
       " 'attention': 94,\n",
       " 'attorney': 95,\n",
       " 'attracted': 96,\n",
       " 'authorities': 97,\n",
       " 'autograph': 98,\n",
       " 'automatic': 99,\n",
       " 'automatically': 100,\n",
       " 'autopilot': 101,\n",
       " 'available': 102,\n",
       " 'aviation': 103,\n",
       " 'aware': 104,\n",
       " 'away': 105,\n",
       " 'awful': 106,\n",
       " 'awfully': 107,\n",
       " 'awkward': 108,\n",
       " 'b': 109,\n",
       " 'babbling': 110,\n",
       " 'baby': 111,\n",
       " 'back': 112,\n",
       " 'backhand': 113,\n",
       " 'bad': 114,\n",
       " 'badfella': 115,\n",
       " 'balance': 116,\n",
       " 'bald': 117,\n",
       " 'ball': 118,\n",
       " 'balloon': 119,\n",
       " 'balm': 120,\n",
       " 'band': 121,\n",
       " 'barely': 122,\n",
       " 'barricade': 123,\n",
       " 'barrier': 124,\n",
       " 'barry': 125,\n",
       " 'base': 126,\n",
       " 'bats': 127,\n",
       " 'battery': 128,\n",
       " 'be': 129,\n",
       " 'beams': 130,\n",
       " 'bear': 131,\n",
       " 'beards': 132,\n",
       " 'bears': 133,\n",
       " 'beast': 134,\n",
       " 'beautiful': 135,\n",
       " 'because': 136,\n",
       " 'bed': 137,\n",
       " 'bedbug': 138,\n",
       " 'bee': 139,\n",
       " 'beekeeper': 140,\n",
       " 'beekeepers': 141,\n",
       " 'been': 142,\n",
       " 'beep': 143,\n",
       " 'beer': 144,\n",
       " 'bees': 145,\n",
       " 'before': 146,\n",
       " 'begin': 147,\n",
       " 'begins': 148,\n",
       " 'behind': 149,\n",
       " 'being': 150,\n",
       " 'bejesus': 151,\n",
       " 'belated': 152,\n",
       " 'believe': 153,\n",
       " 'believed': 154,\n",
       " 'belong': 155,\n",
       " 'benefit': 156,\n",
       " 'benson': 157,\n",
       " 'bent': 158,\n",
       " 'best': 159,\n",
       " 'bet': 160,\n",
       " 'better': 161,\n",
       " 'between': 162,\n",
       " 'big': 163,\n",
       " 'billion': 164,\n",
       " 'birds': 165,\n",
       " 'birth': 166,\n",
       " 'bit': 167,\n",
       " 'biting': 168,\n",
       " 'black': 169,\n",
       " 'blacktop': 170,\n",
       " 'blade': 171,\n",
       " 'blend': 172,\n",
       " 'blew': 173,\n",
       " 'blinked': 174,\n",
       " 'blood': 175,\n",
       " 'bloome': 176,\n",
       " 'blossom': 177,\n",
       " 'blow': 178,\n",
       " 'blows': 179,\n",
       " 'blue': 180,\n",
       " 'bluffing': 181,\n",
       " 'boat': 182,\n",
       " 'bob': 183,\n",
       " 'body': 184,\n",
       " 'bogus': 185,\n",
       " 'book': 186,\n",
       " 'boots': 187,\n",
       " 'bored': 188,\n",
       " 'born': 189,\n",
       " 'borrow': 190,\n",
       " 'both': 191,\n",
       " 'bothering': 192,\n",
       " 'bottom': 193,\n",
       " 'bounty': 194,\n",
       " 'bouquets': 195,\n",
       " 'bowl': 196,\n",
       " 'box': 197,\n",
       " 'boy': 198,\n",
       " 'boys': 199,\n",
       " 'brain': 200,\n",
       " 'brave': 201,\n",
       " 'brazenly': 202,\n",
       " 'bread': 203,\n",
       " 'break': 204,\n",
       " 'breakfast': 205,\n",
       " 'breaking': 206,\n",
       " 'breath': 207,\n",
       " 'breathe': 208,\n",
       " 'breaths': 209,\n",
       " 'bred': 210,\n",
       " 'briefcase': 211,\n",
       " 'bright': 212,\n",
       " 'bring': 213,\n",
       " 'bringing': 214,\n",
       " 'britches': 215,\n",
       " 'brochure': 216,\n",
       " 'brooch': 217,\n",
       " 'brooms': 218,\n",
       " 'bubble': 219,\n",
       " 'bud': 220,\n",
       " 'buddy': 221,\n",
       " 'bug': 222,\n",
       " 'bugging': 223,\n",
       " 'bugs': 224,\n",
       " 'build': 225,\n",
       " 'building': 226,\n",
       " 'bumble': 227,\n",
       " 'bumbleton': 228,\n",
       " 'business': 229,\n",
       " 'businesses': 230,\n",
       " 'busted': 231,\n",
       " 'busy': 232,\n",
       " 'but': 233,\n",
       " 'buttocks': 234,\n",
       " 'buzz': 235,\n",
       " 'buzzwell': 236,\n",
       " 'buzzy': 237,\n",
       " 'by': 238,\n",
       " 'bye': 239,\n",
       " 'cab': 240,\n",
       " 'cafeteria': 241,\n",
       " 'cake': 242,\n",
       " 'call': 243,\n",
       " 'called': 244,\n",
       " 'calories': 245,\n",
       " 'campaign': 246,\n",
       " 'camps': 247,\n",
       " 'can': 248,\n",
       " 'candy': 249,\n",
       " 'cannot': 250,\n",
       " 'capable': 251,\n",
       " 'captain': 252,\n",
       " 'capture': 253,\n",
       " 'car': 254,\n",
       " 'card': 255,\n",
       " 'care': 256,\n",
       " 'career': 257,\n",
       " 'careful': 258,\n",
       " 'carefully': 259,\n",
       " 'carob': 260,\n",
       " 'case': 261,\n",
       " 'casually': 262,\n",
       " 'catch': 263,\n",
       " 'cause': 264,\n",
       " 'celebrate': 265,\n",
       " 'celebrating': 266,\n",
       " 'celery': 267,\n",
       " 'center': 268,\n",
       " 'century': 269,\n",
       " 'ceremonies': 270,\n",
       " 'chance': 271,\n",
       " 'change': 272,\n",
       " 'changes': 273,\n",
       " 'charges': 274,\n",
       " 'check': 275,\n",
       " 'cheering': 276,\n",
       " 'cheese': 277,\n",
       " 'children': 278,\n",
       " 'chill': 279,\n",
       " 'chips': 280,\n",
       " 'choice': 281,\n",
       " 'choices': 282,\n",
       " 'choose': 283,\n",
       " 'chopsticks': 284,\n",
       " 'church': 285,\n",
       " 'churning': 286,\n",
       " 'chute': 287,\n",
       " 'cicada': 288,\n",
       " 'cinnamon': 289,\n",
       " 'circular': 290,\n",
       " 'circumstances': 291,\n",
       " 'class': 292,\n",
       " 'clean': 293,\n",
       " 'clear': 294,\n",
       " 'clients': 295,\n",
       " 'close': 296,\n",
       " 'closed': 297,\n",
       " 'clothes': 298,\n",
       " 'cloudy': 299,\n",
       " 'coaster': 300,\n",
       " 'cockpit': 301,\n",
       " 'coffee': 302,\n",
       " 'collectively': 303,\n",
       " 'collector': 304,\n",
       " 'college': 305,\n",
       " 'color': 306,\n",
       " 'colored': 307,\n",
       " 'combined': 308,\n",
       " 'come': 309,\n",
       " 'comes': 310,\n",
       " 'coming': 311,\n",
       " 'common': 312,\n",
       " 'community': 313,\n",
       " 'compadres': 314,\n",
       " 'companies': 315,\n",
       " 'company': 316,\n",
       " 'compelling': 317,\n",
       " 'compete': 318,\n",
       " 'competition': 319,\n",
       " 'complete': 320,\n",
       " 'completely': 321,\n",
       " 'comrades': 322,\n",
       " 'concentrate': 323,\n",
       " 'concludes': 324,\n",
       " 'congratulations': 325,\n",
       " 'consider': 326,\n",
       " 'considered': 327,\n",
       " 'conspiracy': 328,\n",
       " 'constantly': 329,\n",
       " 'contoured': 330,\n",
       " 'contraption': 331,\n",
       " 'control': 332,\n",
       " 'controls': 333,\n",
       " 'coolest': 334,\n",
       " 'cooling': 335,\n",
       " 'coordinator': 336,\n",
       " 'copilot': 337,\n",
       " 'copy': 338,\n",
       " 'corrected': 339,\n",
       " 'correctly': 340,\n",
       " 'cotton': 341,\n",
       " 'couch': 342,\n",
       " 'could': 343,\n",
       " 'couldn': 344,\n",
       " 'counting': 345,\n",
       " 'county': 346,\n",
       " 'couple': 347,\n",
       " 'course': 348,\n",
       " 'coursing': 349,\n",
       " 'court': 350,\n",
       " 'cousin': 351,\n",
       " 'cousins': 352,\n",
       " 'covered': 353,\n",
       " 'crashing': 354,\n",
       " 'crazy': 355,\n",
       " 'cream': 356,\n",
       " 'creatures': 357,\n",
       " 'creep': 358,\n",
       " 'crew': 359,\n",
       " 'cricket': 360,\n",
       " 'crime': 361,\n",
       " 'crossed': 362,\n",
       " 'crowds': 363,\n",
       " 'crud': 364,\n",
       " 'crumb': 365,\n",
       " 'cry': 366,\n",
       " 'culture': 367,\n",
       " 'cup': 368,\n",
       " 'cups': 369,\n",
       " 'cut': 370,\n",
       " 'd': 371,\n",
       " 'da': 372,\n",
       " 'dad': 373,\n",
       " 'dada': 374,\n",
       " 'daffodil': 375,\n",
       " 'daisies': 376,\n",
       " 'damage': 377,\n",
       " 'dangerous': 378,\n",
       " 'dash': 379,\n",
       " 'date': 380,\n",
       " 'dated': 381,\n",
       " 'dating': 382,\n",
       " 'dave': 383,\n",
       " 'dawg': 384,\n",
       " 'day': 385,\n",
       " 'days': 386,\n",
       " 'dead': 387,\n",
       " 'deadified': 388,\n",
       " 'deady': 389,\n",
       " 'deal': 390,\n",
       " 'dean': 391,\n",
       " 'death': 392,\n",
       " 'decide': 393,\n",
       " 'decision': 394,\n",
       " 'decisions': 395,\n",
       " 'deck': 396,\n",
       " 'dee': 397,\n",
       " 'degrees': 398,\n",
       " 'delay': 399,\n",
       " 'deli': 400,\n",
       " 'demand': 401,\n",
       " 'denouncing': 402,\n",
       " 'deny': 403,\n",
       " 'depends': 404,\n",
       " 'desk': 405,\n",
       " 'destruction': 406,\n",
       " 'detail': 407,\n",
       " 'developing': 408,\n",
       " 'devilishly': 409,\n",
       " 'diabolical': 410,\n",
       " 'did': 411,\n",
       " 'didn': 412,\n",
       " 'die': 413,\n",
       " 'died': 414,\n",
       " 'difference': 415,\n",
       " 'different': 416,\n",
       " 'difficult': 417,\n",
       " 'dirty': 418,\n",
       " 'disaster': 419,\n",
       " 'disconcerting': 420,\n",
       " 'discussing': 421,\n",
       " 'disease': 422,\n",
       " 'dismissal': 423,\n",
       " 'distant': 424,\n",
       " 'distinctive': 425,\n",
       " 'distinguished': 426,\n",
       " 'disturbing': 427,\n",
       " 'divine': 428,\n",
       " 'division': 429,\n",
       " 'do': 430,\n",
       " 'doctor': 431,\n",
       " 'doctored': 432,\n",
       " 'documentary': 433,\n",
       " 'does': 434,\n",
       " 'doesn': 435,\n",
       " 'doggone': 436,\n",
       " 'dogs': 437,\n",
       " 'doing': 438,\n",
       " 'dollar': 439,\n",
       " 'don': 440,\n",
       " 'done': 441,\n",
       " 'dots': 442,\n",
       " 'down': 443,\n",
       " 'downstairs': 444,\n",
       " 'downtown': 445,\n",
       " 'drag': 446,\n",
       " 'dragonfly': 447,\n",
       " 'drain': 448,\n",
       " 'drapes': 449,\n",
       " 'dream': 450,\n",
       " 'dreaming': 451,\n",
       " 'dress': 452,\n",
       " 'dressed': 453,\n",
       " 'drive': 454,\n",
       " 'driving': 455,\n",
       " 'drop': 456,\n",
       " 'dude': 457,\n",
       " 'dumb': 458,\n",
       " 'during': 459,\n",
       " 'dustbuster': 460,\n",
       " 'dying': 461,\n",
       " 'dynamite': 462,\n",
       " 'ear': 463,\n",
       " 'earned': 464,\n",
       " 'earth': 465,\n",
       " 'easy': 466,\n",
       " 'eat': 467,\n",
       " 'eating': 468,\n",
       " 'ecstasy': 469,\n",
       " 'eight': 470,\n",
       " 'eighth': 471,\n",
       " 'elastic': 472,\n",
       " 'elected': 473,\n",
       " 'em': 474,\n",
       " 'emmy': 475,\n",
       " 'emotion': 476,\n",
       " 'emotional': 477,\n",
       " 'employ': 478,\n",
       " 'employment': 479,\n",
       " 'end': 480,\n",
       " 'engines': 481,\n",
       " 'enjoy': 482,\n",
       " 'enough': 483,\n",
       " 'entire': 484,\n",
       " 'equals': 485,\n",
       " 'er': 486,\n",
       " 'ers': 487,\n",
       " 'even': 488,\n",
       " 'evening': 489,\n",
       " 'events': 490,\n",
       " 'ever': 491,\n",
       " 'every': 492,\n",
       " 'everybody': 493,\n",
       " 'everyone': 494,\n",
       " 'everything': 495,\n",
       " 'everywhere': 496,\n",
       " 'evidence': 497,\n",
       " 'exactly': 498,\n",
       " 'example': 499,\n",
       " 'except': 500,\n",
       " 'excited': 501,\n",
       " 'excuse': 502,\n",
       " 'exhausting': 503,\n",
       " 'existence': 504,\n",
       " 'experience': 505,\n",
       " 'explain': 506,\n",
       " 'exploded': 507,\n",
       " 'exploiting': 508,\n",
       " 'eye': 509,\n",
       " 'eyes': 510,\n",
       " 'face': 511,\n",
       " 'faces': 512,\n",
       " 'fact': 513,\n",
       " 'faculty': 514,\n",
       " 'fairy': 515,\n",
       " 'fake': 516,\n",
       " 'falls': 517,\n",
       " 'familiar': 518,\n",
       " 'fantastic': 519,\n",
       " 'far': 520,\n",
       " 'farm': 521,\n",
       " 'farms': 522,\n",
       " 'fashion': 523,\n",
       " 'fast': 524,\n",
       " 'faster': 525,\n",
       " 'fat': 526,\n",
       " 'fatal': 527,\n",
       " 'father': 528,\n",
       " 'fault': 529,\n",
       " 'favor': 530,\n",
       " 'favorite': 531,\n",
       " 'feel': 532,\n",
       " 'feeling': 533,\n",
       " 'felled': 534,\n",
       " 'fellow': 535,\n",
       " 'felt': 536,\n",
       " 'few': 537,\n",
       " 'fiasco': 538,\n",
       " 'field': 539,\n",
       " 'filthy': 540,\n",
       " 'final': 541,\n",
       " 'finally': 542,\n",
       " 'find': 543,\n",
       " 'finds': 544,\n",
       " 'fine': 545,\n",
       " 'finish': 546,\n",
       " 'first': 547,\n",
       " 'fit': 548,\n",
       " 'five': 549,\n",
       " 'flabbergasted': 550,\n",
       " 'flames': 551,\n",
       " 'flayman': 552,\n",
       " 'flew': 553,\n",
       " 'flies': 554,\n",
       " 'flight': 555,\n",
       " 'float': 556,\n",
       " 'floats': 557,\n",
       " 'floral': 558,\n",
       " 'florist': 559,\n",
       " 'flower': 560,\n",
       " 'flowered': 561,\n",
       " 'flowers': 562,\n",
       " 'flush': 563,\n",
       " 'fly': 564,\n",
       " 'flying': 565,\n",
       " 'fold': 566,\n",
       " 'folds': 567,\n",
       " 'follow': 568,\n",
       " 'food': 569,\n",
       " 'fool': 570,\n",
       " 'for': 571,\n",
       " 'force': 572,\n",
       " 'forcibly': 573,\n",
       " 'forever': 574,\n",
       " 'forget': 575,\n",
       " 'former': 576,\n",
       " 'formula': 577,\n",
       " 'forward': 578,\n",
       " 'fozzie': 579,\n",
       " 'frankie': 580,\n",
       " 'freak': 581,\n",
       " 'freaks': 582,\n",
       " 'free': 583,\n",
       " 'freeze': 584,\n",
       " 'fresh': 585,\n",
       " 'fried': 586,\n",
       " 'friend': 587,\n",
       " 'friends': 588,\n",
       " 'from': 589,\n",
       " 'front': 590,\n",
       " 'frosting': 591,\n",
       " 'fruits': 592,\n",
       " 'ftd': 593,\n",
       " 'full': 594,\n",
       " 'fun': 595,\n",
       " 'functioning': 596,\n",
       " 'funeral': 597,\n",
       " 'funny': 598,\n",
       " 'fuzz': 599,\n",
       " 'fuzzy': 600,\n",
       " 'gallons': 601,\n",
       " 'game': 602,\n",
       " 'games': 603,\n",
       " 'gandhi': 604,\n",
       " 'ganic': 605,\n",
       " 'garnishments': 606,\n",
       " 'gate': 607,\n",
       " 'gel': 608,\n",
       " 'genius': 609,\n",
       " 'gentlemen': 610,\n",
       " 'get': 611,\n",
       " 'gets': 612,\n",
       " 'getting': 613,\n",
       " 'giant': 614,\n",
       " 'gift': 615,\n",
       " 'girl': 616,\n",
       " 'girlfriend': 617,\n",
       " 'girls': 618,\n",
       " 'give': 619,\n",
       " 'giving': 620,\n",
       " 'glad': 621,\n",
       " 'glasses': 622,\n",
       " 'global': 623,\n",
       " 'glorification': 624,\n",
       " 'glow': 625,\n",
       " 'gnarly': 626,\n",
       " 'go': 627,\n",
       " 'god': 628,\n",
       " 'goes': 629,\n",
       " 'going': 630,\n",
       " 'gold': 631,\n",
       " 'golden': 632,\n",
       " 'gone': 633,\n",
       " 'gonna': 634,\n",
       " 'good': 635,\n",
       " 'goodbye': 636,\n",
       " 'goodfella': 637,\n",
       " 'goodness': 638,\n",
       " 'gordon': 639,\n",
       " 'got': 640,\n",
       " 'gotta': 641,\n",
       " 'gotten': 642,\n",
       " 'grab': 643,\n",
       " 'grabby': 644,\n",
       " 'grade': 645,\n",
       " 'graduate': 646,\n",
       " 'graduating': 647,\n",
       " 'graduation': 648,\n",
       " 'grandmother': 649,\n",
       " 'granny': 650,\n",
       " 'grasshopper': 651,\n",
       " 'grateful': 652,\n",
       " 'great': 653,\n",
       " 'greater': 654,\n",
       " 'greatest': 655,\n",
       " 'gross': 656,\n",
       " 'ground': 657,\n",
       " 'group': 658,\n",
       " 'growing': 659,\n",
       " 'guatemalan': 660,\n",
       " 'guess': 661,\n",
       " 'guest': 662,\n",
       " 'gun': 663,\n",
       " 'gusty': 664,\n",
       " 'guy': 665,\n",
       " 'guys': 666,\n",
       " 'had': 667,\n",
       " 'hair': 668,\n",
       " 'hairy': 669,\n",
       " 'hal': 670,\n",
       " 'half': 671,\n",
       " 'hallelujah': 672,\n",
       " 'hands': 673,\n",
       " 'handsome': 674,\n",
       " 'hang': 675,\n",
       " 'hangs': 676,\n",
       " 'haphazardly': 677,\n",
       " 'happened': 678,\n",
       " 'happening': 679,\n",
       " 'happens': 680,\n",
       " 'happiest': 681,\n",
       " 'happy': 682,\n",
       " 'hard': 683,\n",
       " 'harder': 684,\n",
       " 'harmless': 685,\n",
       " 'harrys': 686,\n",
       " 'has': 687,\n",
       " 'hat': 688,\n",
       " 'hate': 689,\n",
       " 'have': 690,\n",
       " 'haven': 691,\n",
       " 'having': 692,\n",
       " 'he': 693,\n",
       " 'head': 694,\n",
       " 'headed': 695,\n",
       " 'heads': 696,\n",
       " 'health': 697,\n",
       " 'hear': 698,\n",
       " 'heard': 699,\n",
       " 'heart': 700,\n",
       " 'heat': 701,\n",
       " 'heating': 702,\n",
       " 'heaving': 703,\n",
       " 'hector': 704,\n",
       " 'hello': 705,\n",
       " 'helmet': 706,\n",
       " 'help': 707,\n",
       " 'helping': 708,\n",
       " 'helpless': 709,\n",
       " 'her': 710,\n",
       " 'hercules': 711,\n",
       " 'here': 712,\n",
       " 'hexagon': 713,\n",
       " 'hey': 714,\n",
       " 'hi': 715,\n",
       " 'high': 716,\n",
       " 'him': 717,\n",
       " 'his': 718,\n",
       " 'history': 719,\n",
       " 'hit': 720,\n",
       " 'hitchhiked': 721,\n",
       " 'hitting': 722,\n",
       " 'hive': 723,\n",
       " 'hivo': 724,\n",
       " 'hockey': 725,\n",
       " 'hold': 726,\n",
       " 'hollywood': 727,\n",
       " 'holographic': 728,\n",
       " 'home': 729,\n",
       " 'homes': 730,\n",
       " 'honesco': 731,\n",
       " 'honex': 732,\n",
       " 'honey': 733,\n",
       " 'honeybee': 734,\n",
       " 'honeybees': 735,\n",
       " 'honeyburton': 736,\n",
       " 'honor': 737,\n",
       " 'honorable': 738,\n",
       " 'honron': 739,\n",
       " 'hope': 740,\n",
       " 'hoping': 741,\n",
       " 'horrible': 742,\n",
       " 'hospitals': 743,\n",
       " 'hot': 744,\n",
       " 'hothead': 745,\n",
       " 'hotter': 746,\n",
       " 'hottest': 747,\n",
       " 'hour': 748,\n",
       " 'hours': 749,\n",
       " 'house': 750,\n",
       " 'hover': 751,\n",
       " 'how': 752,\n",
       " 'huge': 753,\n",
       " 'huh': 754,\n",
       " 'human': 755,\n",
       " 'humans': 756,\n",
       " 'humming': 757,\n",
       " 'hundreds': 758,\n",
       " 'hurry': 759,\n",
       " 'hurt': 760,\n",
       " 'hurts': 761,\n",
       " 'i': 762,\n",
       " 'idea': 763,\n",
       " 'ideas': 764,\n",
       " 'idiots': 765,\n",
       " 'if': 766,\n",
       " 'ignacio': 767,\n",
       " 'iguana': 768,\n",
       " 'illegally': 769,\n",
       " 'illegitimate': 770,\n",
       " 'image': 771,\n",
       " 'imagine': 772,\n",
       " 'imagines': 773,\n",
       " 'important': 774,\n",
       " 'impose': 775,\n",
       " 'impossible': 776,\n",
       " 'improve': 777,\n",
       " 'in': 778,\n",
       " 'incapacitated': 779,\n",
       " 'inclusion': 780,\n",
       " 'incorporating': 781,\n",
       " 'incredible': 782,\n",
       " 'individuals': 783,\n",
       " 'industries': 784,\n",
       " 'industry': 785,\n",
       " 'inflatable': 786,\n",
       " 'information': 787,\n",
       " 'initial': 788,\n",
       " 'inner': 789,\n",
       " 'insane': 790,\n",
       " 'insect': 791,\n",
       " 'inside': 792,\n",
       " 'inspector': 793,\n",
       " 'instead': 794,\n",
       " 'instinct': 795,\n",
       " 'intend': 796,\n",
       " 'intended': 797,\n",
       " 'intends': 798,\n",
       " 'interest': 799,\n",
       " 'interested': 800,\n",
       " 'interesting': 801,\n",
       " 'interview': 802,\n",
       " 'into': 803,\n",
       " 'intrigues': 804,\n",
       " 'invented': 805,\n",
       " 'is': 806,\n",
       " 'ish': 807,\n",
       " 'isn': 808,\n",
       " 'issues': 809,\n",
       " 'it': 810,\n",
       " 'italian': 811,\n",
       " 'its': 812,\n",
       " 'j': 813,\n",
       " 'jammed': 814,\n",
       " 'janet': 815,\n",
       " 'jar': 816,\n",
       " 'jars': 817,\n",
       " 'jazz': 818,\n",
       " 'jealousy': 819,\n",
       " 'jeanette': 820,\n",
       " 'jell': 821,\n",
       " 'jerks': 822,\n",
       " 'jewish': 823,\n",
       " 'jfk': 824,\n",
       " 'job': 825,\n",
       " 'jobs': 826,\n",
       " 'jock': 827,\n",
       " 'jocks': 828,\n",
       " 'john': 829,\n",
       " 'joke': 830,\n",
       " 'judge': 831,\n",
       " 'jump': 832,\n",
       " 'jury': 833,\n",
       " 'just': 834,\n",
       " 'kachoo': 835,\n",
       " 'kasell': 836,\n",
       " 'keep': 837,\n",
       " 'ken': 838,\n",
       " 'kenneth': 839,\n",
       " 'kenny': 840,\n",
       " 'kept': 841,\n",
       " 'key': 842,\n",
       " 'keychain': 843,\n",
       " 'keychains': 844,\n",
       " 'kick': 845,\n",
       " 'kid': 846,\n",
       " 'kidding': 847,\n",
       " 'kill': 848,\n",
       " 'killer': 849,\n",
       " 'kills': 850,\n",
       " 'kind': 851,\n",
       " 'king': 852,\n",
       " 'kingdom': 853,\n",
       " 'kite': 854,\n",
       " 'klauss': 855,\n",
       " 'knee': 856,\n",
       " 'knew': 857,\n",
       " 'knock': 858,\n",
       " 'knocking': 859,\n",
       " 'knocks': 860,\n",
       " 'know': 861,\n",
       " 'known': 862,\n",
       " 'knows': 863,\n",
       " 'koo': 864,\n",
       " 'korean': 865,\n",
       " 'krelman': 866,\n",
       " 'la': 867,\n",
       " 'label': 868,\n",
       " 'ladies': 869,\n",
       " 'land': 870,\n",
       " 'larry': 871,\n",
       " 'larvi': 872,\n",
       " 'laser': 873,\n",
       " 'last': 874,\n",
       " 'late': 875,\n",
       " 'latest': 876,\n",
       " 'laugh': 877,\n",
       " 'launch': 878,\n",
       " 'lauren': 879,\n",
       " 'law': 880,\n",
       " 'laws': 881,\n",
       " 'lawsuit': 882,\n",
       " 'lawyer': 883,\n",
       " 'lawyers': 884,\n",
       " 'laying': 885,\n",
       " 'layton': 886,\n",
       " 'leader': 887,\n",
       " 'leans': 888,\n",
       " 'learn': 889,\n",
       " 'least': 890,\n",
       " 'leave': 891,\n",
       " 'leaving': 892,\n",
       " 'left': 893,\n",
       " 'legal': 894,\n",
       " 'legally': 895,\n",
       " 'legitimate': 896,\n",
       " 'legs': 897,\n",
       " 'less': 898,\n",
       " 'let': 899,\n",
       " 'letter': 900,\n",
       " 'level': 901,\n",
       " 'life': 902,\n",
       " 'lifesaver': 903,\n",
       " 'lifetime': 904,\n",
       " 'lightning': 905,\n",
       " 'lights': 906,\n",
       " 'like': 907,\n",
       " 'liked': 908,\n",
       " 'likes': 909,\n",
       " 'limit': 910,\n",
       " 'line': 911,\n",
       " 'lines': 912,\n",
       " 'lint': 913,\n",
       " 'liotta': 914,\n",
       " 'lip': 915,\n",
       " 'listen': 916,\n",
       " 'listening': 917,\n",
       " 'little': 918,\n",
       " 'live': 919,\n",
       " 'lived': 920,\n",
       " 'lives': 921,\n",
       " 'living': 922,\n",
       " 'll': 923,\n",
       " 'loaded': 924,\n",
       " 'long': 925,\n",
       " 'longer': 926,\n",
       " 'look': 927,\n",
       " 'looking': 928,\n",
       " 'looks': 929,\n",
       " 'lord': 930,\n",
       " 'lordy': 931,\n",
       " 'lose': 932,\n",
       " 'losing': 933,\n",
       " 'lost': 934,\n",
       " 'lot': 935,\n",
       " 'lou': 936,\n",
       " 'lovable': 937,\n",
       " 'love': 938,\n",
       " 'loving': 939,\n",
       " 'luckily': 940,\n",
       " 'lucky': 941,\n",
       " 'lunch': 942,\n",
       " 'lust': 943,\n",
       " 'm': 944,\n",
       " 'ma': 945,\n",
       " 'machine': 946,\n",
       " 'machines': 947,\n",
       " 'made': 948,\n",
       " 'madison': 949,\n",
       " 'magazine': 950,\n",
       " 'magic': 951,\n",
       " 'major': 952,\n",
       " 'make': 953,\n",
       " 'makes': 954,\n",
       " 'making': 955,\n",
       " 'mall': 956,\n",
       " 'mama': 957,\n",
       " 'mamma': 958,\n",
       " 'man': 959,\n",
       " 'manhattan': 960,\n",
       " 'many': 961,\n",
       " 'marry': 962,\n",
       " 'marshal': 963,\n",
       " 'martin': 964,\n",
       " 'mass': 965,\n",
       " 'massive': 966,\n",
       " 'match': 967,\n",
       " 'matter': 968,\n",
       " 'matters': 969,\n",
       " 'mattresses': 970,\n",
       " 'may': 971,\n",
       " 'maybe': 972,\n",
       " 'mayday': 973,\n",
       " 'me': 974,\n",
       " 'meadow': 975,\n",
       " 'mean': 976,\n",
       " 'means': 977,\n",
       " 'meant': 978,\n",
       " 'meat': 979,\n",
       " 'meet': 980,\n",
       " 'meeting': 981,\n",
       " 'men': 982,\n",
       " 'mercy': 983,\n",
       " 'met': 984,\n",
       " 'metal': 985,\n",
       " 'mia': 986,\n",
       " 'micrograms': 987,\n",
       " 'might': 988,\n",
       " 'mighty': 989,\n",
       " 'miles': 990,\n",
       " 'milk': 991,\n",
       " 'million': 992,\n",
       " 'millions': 993,\n",
       " 'mind': 994,\n",
       " 'minds': 995,\n",
       " 'mine': 996,\n",
       " 'minute': 997,\n",
       " 'minutes': 998,\n",
       " 'miss': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords = np.unique(words)\n",
    "wordIndex = dict((c, i) for i, c in enumerate(uniqueWords))\n",
    "wordIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the actual feauture selection process begins.\n",
    "We set the word limit as 4\n",
    "This specifies that only the previous 4 words will be used to predict the next word.\n",
    "We create two lists, prevW and nextW. prevW stores the previous 4 words and nextW stores its corresponding next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['according', 'to', 'all', 'known'], ['to', 'all', 'known', 'laws'], ['all', 'known', 'laws', 'of'], ['known', 'laws', 'of', 'aviation'], ['laws', 'of', 'aviation', 'there']]\n",
      "\n",
      "\n",
      "['laws', 'of', 'aviation', 'there', 'is']\n"
     ]
    }
   ],
   "source": [
    "wordLim = 4\n",
    "prevW = []\n",
    "nextW = []\n",
    "for i in range(len(words) - wordLim): #Iterating through the 4 less than length of list\n",
    "    prevW.append(words[i:i + wordLim])\n",
    "    nextW.append(words[i + wordLim])\n",
    "print(prevW[0:5])\n",
    "print(\"\\n\")\n",
    "print(nextW[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n",
    "We are going to create two numpy arrays\n",
    "x is for storing the feature\n",
    "y is the corresponding next word\n",
    "We will iterate through x and y, if the word is present, then the corresponding position is made 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "\n",
      "\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(prevW), wordLim, len(uniqueWords)), dtype=bool)\n",
    "y = np.zeros((len(nextW), len(uniqueWords)), dtype=bool)\n",
    "for i,eWords  in enumerate(prevW):\n",
    "    for j, word in enumerate(eWords):\n",
    "        x[i, j, wordIndex[word]] = 1\n",
    "    y[i, wordIndex[nextW[i]]] = 1\n",
    "print(x[0:3][2])\n",
    "print(\"\\n\")\n",
    "print(y[0:3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "We use a single-layer LSTM model with 128 neurons, a fully connected layer, and a softmax function for activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbModel = Sequential()\n",
    "kbModel.add(LSTM(256, input_shape=(wordLim, len(uniqueWords))))\n",
    "kbModel.add(Dense(len(uniqueWords)))\n",
    "kbModel.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "Using an RMSprop optimizer, the model is going to be trained with 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "73/73 [==============================] - 6s 81ms/step - loss: 6.3455 - accuracy: 0.0456 - val_loss: 5.8740 - val_accuracy: 0.1041\n",
      "Epoch 2/4\n",
      "73/73 [==============================] - 6s 86ms/step - loss: 5.4576 - accuracy: 0.1193 - val_loss: 5.8386 - val_accuracy: 0.1122\n",
      "Epoch 3/4\n",
      "73/73 [==============================] - 6s 82ms/step - loss: 4.7080 - accuracy: 0.1813 - val_loss: 5.9376 - val_accuracy: 0.1184\n",
      "Epoch 4/4\n",
      "73/73 [==============================] - 6s 88ms/step - loss: 3.8322 - accuracy: 0.2774 - val_loss: 6.2401 - val_accuracy: 0.1143\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "kbModel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "hist = kbModel.fit(x, y, validation_split=0.05, batch_size=128, epochs=4, shuffle=True).history #Evaluation results can be seen from variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [6.3454909324646,\n",
       "  5.457568645477295,\n",
       "  4.708028793334961,\n",
       "  3.832205295562744],\n",
       " 'accuracy': [0.045630648732185364,\n",
       "  0.11934997886419296,\n",
       "  0.18133878707885742,\n",
       "  0.27744296193122864],\n",
       " 'val_loss': [5.87396764755249,\n",
       "  5.838569164276123,\n",
       "  5.937586784362793,\n",
       "  6.240148544311523],\n",
       " 'val_accuracy': [0.10408163070678711,\n",
       "  0.11224489659070969,\n",
       "  0.11836734414100647,\n",
       "  0.11428571492433548]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "First we need to create a function that encodes the given input after it removes the punctuations and numbers from the input\n",
    "then we need to create a function that chooses the top 5 best predictions made by the model\n",
    "Finally, we create a function that uses the model to predict and then returns 5 possible words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaner(inp,check=True ):\n",
    "    inp= inp.lower()\n",
    "    inp = tokenizer.tokenize(inp)\n",
    "    inp = [item for item in inp if item.isalpha()] \n",
    "    if check==True:\n",
    "        return \" \".join(inp[0:4])\n",
    "    else:\n",
    "        return \" \".join(inp[-4:])\n",
    "\n",
    "def inputEncoder(string,check=True):\n",
    "    string = textCleaner(string,check)\n",
    "    x = np.zeros((1, wordLim, len(uniqueWords)))\n",
    "    print(\"Resulted Sequence\\n\")\n",
    "    for t, word in enumerate(string.split()):\n",
    "        print(word)\n",
    "        x[0, t, wordIndex[word]] = 1\n",
    "    return x\n",
    "\n",
    "def bestResult(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    return heapq.nlargest(5, range(len(preds)), preds.take)\n",
    "\n",
    "def predictor(string,check=True):\n",
    "    if string == \"\":\n",
    "        return(\"0\")\n",
    "    x = inputEncoder(string,check)\n",
    "    prediction = kbModel.predict(x, verbose=0)[0]\n",
    "    nextInd = bestResult(prediction)\n",
    "    return [uniqueWords[idx] for idx in nextInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulted Sequence\n",
      "\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputEncoder(\"Hello how are you doing\".lower(),False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets see how our predictor works.\n",
    "Using tokenizer, we remove the punctuations. Then we remove the numbers. Then we choose the first four words to make the prediction and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence is:  Wait. One of these flowers seems to be on the move.\n",
      "Resulted Sequence\n",
      "\n",
      "wait\n",
      "one\n",
      "of\n",
      "these\n",
      "The possible words can be:  ['flowers', 'bees', 'life', 'roses', 'these']\n"
     ]
    }
   ],
   "source": [
    "inp=\"Wait. One of these flowers seems to be on the move.\"\n",
    "print(\"sequence is: \",inp)\n",
    "print(\"The possible words can be: \",predictor(inp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence is:  picking up a lot of bright yellow .\n",
      "Resulted Sequence\n",
      "\n",
      "lot\n",
      "of\n",
      "bright\n",
      "yellow\n",
      "The possible words can be:  ['let', 'oould', 'yellow', 'as', 've']\n"
     ]
    }
   ],
   "source": [
    "inp=\"picking up a lot of bright yellow .\"\n",
    "print(\"sequence is: \",inp)\n",
    "print(\"The possible words can be: \",predictor(inp,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence is:  Are bees really dead or not\n",
      "Resulted Sequence\n",
      "\n",
      "really\n",
      "dead\n",
      "or\n",
      "not\n",
      "The possible words can be:  ['dead', 'the', 'all', 'at', 'there']\n"
     ]
    }
   ],
   "source": [
    "inp=\"Are bees really dead or not\"\n",
    "print(\"sequence is: \",inp)\n",
    "print(\"The possible words can be: \",predictor(inp,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
